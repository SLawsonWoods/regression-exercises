{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Module Recap\n",
    "\n",
    "- [Overall Presentation Feedback](#Overall-Presentation-Feedback)\n",
    "- [Pipeline Review](#Pipeline-Review)\n",
    "- [Tips and Tricks](#Tips-and-Tricks)\n",
    "- [Going Further](#Going-Further)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Presentation Feedback\n",
    "\n",
    "\n",
    "- Talk through your visuals more: what is this? what are we looking at? why is this in the presentation? what is the takeaway? Remember your audience has never seen this viz before, even though you have probably been staring at it tweaking it for hours\n",
    "- reiterate where the data comes from, size, prep, etc (doesn't have to be long) (e.g. our database contains... I used ...)\n",
    "- communicate model performance is on unseen data\n",
    "- executive summary -- overall good -- quantify model performance, drivers\n",
    "- show don't tell, if a viz can explain it, it's generally preferable -- e.g. model performance comparison\n",
    "- include answers to questions\n",
    "- don't use animation; competes w/ attention; doesn't add anything; looks funky over zoom screen share (or in-person via an adapter!)\n",
    "\n",
    "---\n",
    "\n",
    "- Number Formatting: in tables, slides, visualizations\n",
    "- use a diverging color palette for heatmaps, set anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Review\n",
    "\n",
    "What happens when?\n",
    "\n",
    "- SQL: acquire + prep\n",
    "- drop outliers: prep\n",
    "- Python: all over the place\n",
    "- **imputation**: filling in missing values (w/ e.g. 0, or the average value): prep\n",
    "- Pandas: all over the place\n",
    "- train test split: prep -- before explore\n",
    "- scaling: prep/modeling -- have a prep function that returns scaled and unscaled data\n",
    "- hypothesis testing: explore\n",
    "- visualization: all over the pipeline\n",
    "- touching the test data: *only* at the end\n",
    "    - scaling + imputation parameters are calculated from the **training** split\n",
    "    - any data prep parameters are calculated from the training split\n",
    "        - drop outliers -- any data point > 3 stds from the mean\n",
    "        - mean + std are calculated from train, not test, not the whole dataset\n",
    "    - before being fed into a model, test data is prepped in the same way the training data was\n",
    "    - as far as prep goes -- if you could do it to a single new data point, you can do it to the test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVP Pipeline Example\n",
    "\n",
    "1. **Acquire**: acquire from SQL\n",
    "1. **Prepare**: drop nulls; train_test_split\n",
    "1. **Explore**: visualize each feature against the target\n",
    "1. **Model**: calculate a baseline; fit a linear regression model on 2 features; fit a 2nd degree polynomial model; compare performance\n",
    "\n",
    "### Iterate\n",
    "\n",
    "1. Move functions to `wrangle.py`\n",
    "1. Visualize multiple feature interactions\n",
    "1. Stats tests\n",
    "1. scale data and see how model performance is impacted\n",
    "1. Impute nulls\n",
    "1. Drop outliers\n",
    "1. more model types\n",
    "1. Cleanup visualizations\n",
    "1. Tweak model hyperparameters\n",
    "1. More visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting numbers in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(np.random.randn(5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.float_format')\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some fake data for demonstration\n",
    "np.random.seed(123)\n",
    "df = pd.DataFrame()\n",
    "df['x1'] = np.random.randn(100)\n",
    "df['x2'] = np.random.randn(100) * 1_000_000\n",
    "df['x3'] = df.x1 + np.random.randn(100)\n",
    "df['x4'] = - df.x1 + np.random.randn(100)\n",
    "df['x5'] = np.random.randn(100) * 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try also coolwarm, coolwarm_r\n",
    "# see https://matplotlib.org/stable/tutorials/colors/colormaps.html#diverging\n",
    "# vmin, vmax, center\n",
    "sns.heatmap(df.corr(), cmap='PiYG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Axis Ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.x2.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.x2.plot.hist()\n",
    "ax.xaxis.set_major_formatter(lambda x, pos: '{:.1f} m'.format(x / 1_000_000))\n",
    "ax.set(xlabel='x2 (millions)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.x5.plot.hist()\n",
    "ax.xaxis.set_major_formatter('{:,.0f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips['tip_percentage'] = tips.tip / tips.total_bill\n",
    "\n",
    "ax = tips.groupby('time').tip_percentage.mean().plot.bar()\n",
    "plt.xticks(rotation=0)\n",
    "ax.yaxis.set_major_formatter('{:.0%}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas table styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().style.background_gradient(vmin=-1, vmax=1, cmap='coolwarm_r').format('{:.3f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5, random_state=123).style.highlight_max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5, random_state=123).style.format({\n",
    "    'x2': lambda x: '{:.3} million'.format(x / 1_000_000),\n",
    "    'x3': '{:.1%}'.format,\n",
    "    'x5': '{:,.1f}'.format,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- [Matplotlib axis tick formatting](https://matplotlib.org/stable/gallery/ticks_and_spines/tick-formatters.html)\n",
    "- [Matplotlib color pallettes](https://matplotlib.org/stable/tutorials/colors/colormaps.html#diverging)\n",
    "- [Python string formatting docs](https://docs.python.org/3/library/string.html#formatstrings)\n",
    "- [Pandas table styling](https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "- Consider [median absolute error][1] as an evaluation metric. Large outliers, like in the zillow dataset, have a pretty big impact on RMSE.\n",
    "\n",
    "    $ \\mbox{MAE} = \\mbox{med}(|y - \\hat{y}|) $\n",
    "    \n",
    "- Interpret linear regression model coefficients to determine which features the model thinks are most important and specifically how they impact the target.\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import median_absolute_error as mae\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "X, y = tips[['total_bill', 'size']], tips.tip\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)\n",
    "\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "print('Median Absolute Error: ${:.3f}'.format(mae(y_test, lm.predict(X_test))))\n",
    "print()\n",
    "\n",
    "for feature, coef in pd.Series(lm.coef_, index=X_train.columns).iteritems():\n",
    "    print(f'---\\nHolding other features constant,\\nfor every 1 unit increase in {feature}\\nthe model predicts tip amount goes up by ${coef:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
